{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd291ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"historical_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7395245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL : TO PREDICT THE DELIVERY DURATION, I.E., DIFFERENCE BETWEEN CREATED_AT AND ACTUAL_DELIVERY_TIME\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['created_at'] = pd.to_datetime(data['created_at'])\n",
    "data['actual_delivery_time'] = pd.to_datetime(data['actual_delivery_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288043f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['delivery_duration'] = (data['actual_delivery_time'] - data['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['delivery_duration'] = (data['delivery_duration']).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe14d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['busy_dashers_ratio'] = (data['total_busy_dashers']/data['total_onshift_dashers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad666422",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['estimated_non_prep_duration'] = data['estimated_order_place_duration'] + data['estimated_store_to_consumer_driving_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bcea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['market_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a769c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we use this, we'll have too many dummies, which will explode the dataset\n",
    "data['store_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868aa537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# good size for dummies\n",
    "# order protocol is the mode in which doordash receives orders.\n",
    "data['order_protocol'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32197cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_id_dummies = pd.get_dummies(data.market_id)\n",
    "market_id_dummies = market_id_dummies.add_prefix('market_id_')\n",
    "\n",
    "order_protocol_dummies = pd.get_dummies(data.order_protocol)\n",
    "order_protocol_dummies = order_protocol_dummies.add_prefix('order_protocol_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570501ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_id_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ee73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_id_unique = data['store_id'].unique().tolist()\n",
    "store_id_and_category = {store_id : data[data.store_id == store_id].store_primary_category.mode() for store_id\n",
    "                        in store_id_unique}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb5bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill(store_id):\n",
    "    try:\n",
    "        return store_id_and_category[store_id].values[0]\n",
    "    except:\n",
    "        return np.nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e73ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['store_category_without_null'] = data['store_id'].apply(fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d643bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummies for store_category\n",
    "store_category_dummies = pd.get_dummies(data.store_category_without_null)\n",
    "store_category_dummies = store_category_dummies.add_prefix('category_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping a few columns, where we replace them with their dummy columns. \n",
    "\n",
    "train_data = data.drop(columns = ['market_id', 'created_at', 'actual_delivery_time', 'store_id',\n",
    "                                 'store_primary_category', 'store_category_without_null', 'order_protocol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the dummy columns to our training data\n",
    "train_data = pd.concat([train_data, market_id_dummies, store_category_dummies, order_protocol_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25961431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dtype to float for modelling\n",
    "train_data = train_data.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc99a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b51998",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['total_busy_dashers'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the correlation matrix\n",
    "corr = train_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea41647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (25,25)) # size of the graph 25x25 is perfect\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap = True)\n",
    "sns.heatmap(corr, cmap = cmap, vmax = .3, linewidths = 0.5, center = 0, square = True, cbar_kws = {'shrink':.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b517f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redundant_pairs(df):\n",
    "    redundant_pairs = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0,df.shape[1]):\n",
    "        for j in range(0,i+1):\n",
    "            redundant_pairs.add((cols[i],cols[j]))\n",
    "    return redundant_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6da2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_abs_corrs(df, n = 5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    # series object that contains all the pairwise correlations of the columns given.\n",
    "    items_to_drop = get_redundant_pairs(df) #find out the items from the upper triangle and diagonal\n",
    "    au_corr = au_corr.drop(labels = items_to_drop).sort_values(ascending = False)\n",
    "    # drops the excess items (5050 in case of train_data) to make processing faster.\n",
    "    return au_corr[0:n] #returns the n-highest correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93802fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_abs_corrs(train_data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9329dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll drop the highly correlated features\n",
    "train_data = train_data.drop(columns = ['total_onshift_dashers', 'total_busy_dashers',\n",
    "                                       'estimated_non_prep_duration', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b3f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(order_protocol_dummies, axis = 1) # dropping the set of all order_protocol_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1cf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(market_id_dummies, axis = 1) # dropping the set of all market_id_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354459ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_abs_corrs(train_data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineering new features and dropping the ones previously\n",
    "train_data['avg_item_price'] = train_data['subtotal']/train_data['total_items']\n",
    "train_data['order_price_range'] = train_data['max_item_price'] - train_data['min_item_price']\n",
    "train_data['distinct_item_percent_of_total'] = train_data['num_distinct_items']/train_data['total_items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd147d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(columns = ['total_items', 'subtotal', 'max_item_price', 'min_item_price',\n",
    "                                       'num_distinct_items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['category_indonesian'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d8263",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(columns = ['category_indonesian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d26c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_abs_corrs(train_data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff68f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.astype('float32') #converting to suitable datatype\n",
    "# converting infinity values to null values to finally drop them\n",
    "train_data.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "train_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0c725",
   "metadata": {},
   "source": [
    "# we have significantly reduced the collinearity problem so now we can focus on something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll now try to reduce the multicollinearity problem and work on that using VIF and PCA. we'll also use certain\n",
    "# regression techniques to find the importance (gini purity of each feature) to reduce the dimensions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF(variance inflation factor) - basically tests for how good a variable can be approximated based on a linear\n",
    "# combination of all other variables. it is basically like finding the R^2 of the regression having each of the \n",
    "# dependent variables regressed on all the other dependent variables. VIF = 1/1-R^2i, where R^2i is the score for the\n",
    "# regression of the i th dependent variable on all other dependent variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb91b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vif(features):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['features'] = features\n",
    "    vif_data['VIF'] = [variance_inflation_factor(train_data[features].values, i ) for i in range(len(features))]\n",
    "    return vif_data.sort_values(by = ['VIF']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b9ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting features to a list\n",
    "features = train_data.drop(columns = ['delivery_duration']).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff5cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data = compute_vif(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654314ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove features with vif score higher than a certain upper bound\n",
    "multicoll = True\n",
    "\n",
    "while multicoll:\n",
    "    highest_vif_feature = vif_data['features'].values.tolist()[-1]\n",
    "    features.remove(highest_vif_feature)\n",
    "    vif_data = compute_vif(features)\n",
    "    if len(vif_data[vif_data['VIF'] > 15]) == 0 :\n",
    "        multicoll = False\n",
    "    else:\n",
    "        multicoll = True\n",
    "\n",
    "vif_data\n",
    "\n",
    "# we now have effectively 79 columns. the next step is to see the importance of each of these features in predicting\n",
    "# the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will reduce the dimensions further by using PCA and random forests regression to find the most important features\n",
    "# by predicitve capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f963ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = vif_data['features'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8625cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a524df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[selected_features] #reduced feature set i.e. with 79 features\n",
    "y = train_data['delivery_duration'] #target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6916330",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, = train_test_split(x, y, test_size = 0.2, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77580cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = [f'feature {i}' for i in range(x.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5308a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(random_state = 42)\n",
    "forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e6cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {} # a dictionary that holds the feature and their gini importance pairs\n",
    "for feature, importance in zip(x.columns, forest.feature_importances_): # zip pairs up the items in the two iterables listed\n",
    "    feats[feature] = importance #adds the name-value pair\n",
    "\n",
    "importances = pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = importances.from_dict(feats, orient = 'index') # takes the data from the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = importances.rename(columns = {0 : 'gini_importance'}) # renames the column to gini importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = importances.sort_values(by = 'gini_importance') # sorts values in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances.plot(kind = 'bar', rot = 90, figsize = (25,25)) # plots the dataframe\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use PCA to further the dimensionality reduction process. we use 2 types of scalers - minmax and standard.\n",
    "# the problem with these scaling techniques is their high sensitivity to outliers when compared to more robust scalers\n",
    "# like the robustscaler. nevertheless, we continue with the project using this\n",
    "\n",
    "x_train = x_train.values\n",
    "x_train = np.asarray(x_train) # not sure why we need to do this since our data is already in ndarray format\n",
    "\n",
    "x_std = StandardScaler().fit_transform(x_train) # fit trains our ML model. read more about this in your next project\n",
    "\n",
    "pca = PCA().fit(x_std) # gives the PCA of the data fed\n",
    "\n",
    "# visualizing the PCA insights\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlim(0,81,1) # sets the limits on the x-axis\n",
    "plt.xlabel('number of features') # labels the x-axis\n",
    "plt.ylabel('cumulative explained variance') # labels the y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(scaler_type, x, y):\n",
    "    scaler = scaler_type\n",
    "    scaler.fit(x) # not sure why we need to specify the scaler type here if the scaling (i.e. transforming\n",
    "    # isnt actually happening. read on this later.)\n",
    "    x_scaled = scaler.transform(x)\n",
    "    scaler.fit(y.values.reshape(-1,1)) #understand why we need to do the reshaping\n",
    "    y_scaled = scaler.transform(y.values.reshape(-1,1))\n",
    "    \n",
    "    return (x_scaled, y_scaled, scaler, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the scaling function\n",
    "x_scaled, y_scaled, x_scaler, y_scaler = scale(MinMaxScaler(), x, y)\n",
    "x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled = train_test_split(x_scaled, y_scaled, test_size = 0.2,\n",
    "                                                                               random_state = 42)\n",
    "# the randomization gives us the same random entry sets as before since we mentioned the same random_state i.e. 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for scaling the errors back to their pre-transformation units\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse_with_inv_transform(scaler, y_test, y_pred_scaled, model_name):\n",
    "    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1,1))\n",
    "    rmse_error = mean_squared_error(y_test.values.reshape(-1,1)[: , 0], y_pred[: , 0], squared = False)\n",
    "    print('Error = {}'.format(rmse_error) + ' in ' + model_name)\n",
    "    return rmse_error, y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regression(x_train, y_train, x_test, y_test, model, model_name, verbose = False):\n",
    "    model.fit(x_train, y_train) # trains the data according to the model\n",
    "    y_predict = model.predict(x_train) # gives the predictions on the fitted data\n",
    "    train_error = mean_squared_error(y_train, y_predict, squared = False) #rmse for training\n",
    "    y_predict = model.predict(x_test) # gives the predictions on the test data which is not fitted (not sure why \n",
    "    # we arent fitting the test data but let's see that later.)\n",
    "    test_error = mean_squared_error(y_test, y_predict, squared = False) #rmse for testing\n",
    "    if verbose:\n",
    "        print('Train_error = {}'.format(train_error) + ' in ' + model_name)\n",
    "        print('Test_error = {}'.format(test_error) + ' in ' + model_name)\n",
    "    \n",
    "    trained_model = model\n",
    "    \n",
    "    return trained_model, y_predict, train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff698df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {\n",
    "    'regression_model' : [],\n",
    "    'feature_set' : [],\n",
    "    'scaler' : [],\n",
    "    'RMSE' : []\n",
    "}\n",
    "\n",
    "regression_models = {\n",
    "    'Ridge' : linear_model.Ridge(),\n",
    "    'DecisionTree' : tree.DecisionTreeRegressor(max_depth = 6),\n",
    "    'RandomForest' : RandomForestRegressor(),\n",
    "    'XGBoost' : XGBRegressor(),\n",
    "    'MLP' : MLPRegressor()\n",
    "}\n",
    "\n",
    "feature_sets = {\n",
    "    'full_dataset' : importances.index.to_list(),\n",
    "    'top_40_features' : importances.sort_values(by = 'gini_importance')[-40: ].index.to_list(),\n",
    "    'top_20_features' : importances.sort_values(by = 'gini_importance')[-20: ].index.to_list(),\n",
    "    'top_10_features' : importances.sort_values(by = 'gini_importance')[-10: ].index.to_list()    \n",
    "}\n",
    "\n",
    "scalers = {\n",
    "    'standard_scaler' : StandardScaler(),\n",
    "    'MinMax_scaler' : MinMaxScaler(),\n",
    "    'Robust_scaler' : RobustScaler(),\n",
    "    'No_scale' : None\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24bc004",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_set_name in feature_sets.keys():\n",
    "    feature_set = feature_sets[feature_set_name] # returns the set of features accordingly\n",
    "    for scaler_name in scalers.keys():\n",
    "        print(f'scaled with -------{scaler_name}-------using {feature_set_name}')\n",
    "        print('')\n",
    "        for model_name in regression_models.keys():\n",
    "            x = train_data[feature_set]\n",
    "            y = train_data['delivery_duration']\n",
    "            if scaler_name == 'No_scale':\n",
    "                x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "                make_regression(x_train, y_train, x_test, y_test, regression_models[model_name], model_name)\n",
    "            else:\n",
    "                x_scaled, y_scaled, x_scaler, y_scaler = scale(scalers[scaler_name], x, y)\n",
    "                x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
    "                    x_scaled, y_scaled, test_size = 0.2, random_state = 42)\n",
    "                _, y_predict_scaled, _, _ = make_regression(x_train_scaled, y_train_scaled[:, 0], x_test_scaled,\n",
    "                                                           y_test_scaled, regression_models[model_name], model_name)\n",
    "                rmse_error, y_predict = rmse_with_inv_transform(y_scaler, y_test, y_predict_scaled, model_name)\n",
    "            pred_dict['regression_model'].append(model_name)\n",
    "            pred_dict['feature_set'].append(feature_set_name)\n",
    "            pred_dict['scaler'].append(scaler_name)\n",
    "            pred_dict['RMSE'].append(rmse_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_data = pd.DataFrame(pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f44793",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23cdc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_data.plot(kind = 'bar', figsize = (15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5666c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the problem and defining a new variable and estimating that\n",
    "train_data['prep_time'] = train_data['delivery_duration'] - train_data['estimated_order_place_duration'] - train_data['estimated_store_to_consumer_driving_duration']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48532f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the variation in scaling and feature_set sizes doesn't change the output by much, we can just use the most\n",
    "# convenient options. we will use robust scaler and feature_set_of_40.\n",
    "\n",
    "scaler = RobustScaler()\n",
    "feature_set = importances.sort_values(by = 'gini_importance')[-40:].index.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572804c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing the model with the new features on the new variable (i.e. prep_time)\n",
    "\n",
    "print(f'scaled with -------{scaler}-------using top 40 features')\n",
    "print('')\n",
    "for model_name in regression_models.keys():\n",
    "    x = train_data[feature_set].drop(columns = ['estimated_order_place_duration',\n",
    "                                                'estimated_store_to_consumer_driving_duration'])\n",
    "    y = train_data['prep_time']\n",
    "    \n",
    "    x_scaled, y_scaled, x_scaler, y_scaler = scale(scaler, x, y)\n",
    "    x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
    "    x_scaled, y_scaled, test_size = 0.2, random_state = 42)\n",
    "    _, y_predict_scaled, _, _ = make_regression(x_train_scaled, y_train_scaled[:, 0], x_test_scaled,\n",
    "                                                           y_test_scaled, regression_models[model_name], model_name)\n",
    "    rmse_error, y_predict = rmse_with_inv_transform(y_scaler, y_test, y_predict_scaled, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff8168",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_models = {'XGBoost' : XGBRegressor()}\n",
    "\n",
    "print(f'scaled with -------{scaler}-------using top 40 features')\n",
    "print('')\n",
    "for model_name in regression_models.keys():\n",
    "    x = train_data[feature_set].drop(columns = ['estimated_order_place_duration',\n",
    "                                                    'estimated_store_to_consumer_driving_duration'])\n",
    "    y = train_data['prep_time']\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    # get indices of the rows in training and testing datasets\n",
    "    \n",
    "    train_indices = x_train.index\n",
    "    test_indices = x_test.index\n",
    "    \n",
    "    \n",
    "    x_scaled, y_scaled, x_scaler, y_scaler = scale(scaler, x, y)\n",
    "    x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
    "    x_scaled, y_scaled, test_size = 0.2, random_state = 42)\n",
    "    _, y_predict_scaled, _, _ = make_regression(x_train_scaled, y_train_scaled[:, 0], x_test_scaled,\n",
    "                                                               y_test_scaled, regression_models[model_name], model_name)\n",
    "    rmse_error, y_predict = rmse_with_inv_transform(y_scaler, y_test, y_predict_scaled, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea03102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the predicted values in a dictionary\n",
    "predicted_values = {\n",
    "    'total_delivery_duration' : train_data['delivery_duration'][test_indices].values.tolist(),\n",
    "    'prep_duration_prediction' : y_predict.tolist(),\n",
    "    'estimated_order_place_duration' : train_data['estimated_order_place_duration'][test_indices].values.tolist(),\n",
    "    'estimated_store_to_consumer_driving_duration' : train_data['estimated_store_to_consumer_driving_duration'][test_indices].values.tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f829b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERTING THE DICTIONARY TO A DATAFRAME AND DEFINING TH CONCERNED VARIABLE AGAIN\n",
    "values_data = pd.DataFrame.from_dict(predicted_values)\n",
    "values_data\n",
    "values_data['estimated_total_delivery_duration'] = values_data['prep_duration_prediction'] + values_data['estimated_order_place_duration'] + values_data['estimated_store_to_consumer_driving_duration']\n",
    "mean_squared_error(values_data['total_delivery_duration'], values_data['estimated_total_delivery_duration'], squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f58de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING A NEW REGRESSION WITH ONLY THE MOST RELEVANT FEATURES\n",
    "\n",
    "print(f'scaled with -------{scaler}-------using top 3 features')\n",
    "print('')\n",
    "for model_name in regression_models.keys():\n",
    "    x = values_data[['estimated_order_place_duration', 'estimated_store_to_consumer_driving_duration', 'prep_duration_prediction']]\n",
    "    y = values_data['total_delivery_duration']\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    x_scaled, y_scaled, x_scaler, y_scaler = scale(scaler, x, y)\n",
    "    x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
    "    x_scaled, y_scaled, test_size = 0.2, random_state = 42)\n",
    "    _, y_predict_scaled, _, _ = make_regression(x_train_scaled, y_train_scaled[:, 0], x_test_scaled,\n",
    "                                                           y_test_scaled, regression_models[model_name], model_name)\n",
    "    rmse_error, y_predict = rmse_with_inv_transform(y_scaler, y_test, y_predict_scaled, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
